{"cells":[{"cell_type":"markdown","source":["## Get the data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9edba346-678c-4eb5-aad8-630e4b20da9d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["!pip install evaluate datasets sacrebleu accelerate transformers[sentencepiece]"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b9864e59-38f4-4cc4-92c8-6f30ae0c7717","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"66e998a6-8aee-4958-9621-d6d6bb10f427","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Load the dataset\nfrom datasets import load_dataset\ndata = load_dataset(\"pib\", \"en-ta\")\ndata"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6fa048d3-909e-4b4b-a4fc-e4eacd5ce446","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["split_datasets = data[\"train\"].train_test_split(train_size =0.8, seed = 40)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bef91734-6912-41b9-a459-afe0a15b0f62","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from transformers import BertTokenizerFast\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\ndef preprocess(example):\n  #english_text = [example[val][\"translation\"][\"en\"].split() for val in range(len(example))]\n  #tamil_text = [example[val][\"translation\"][\"ta\"].split() for val in range(len(example))]\n  english_text = [ex[\"en\"] for ex in example[\"translation\"]]\n  tamil_text =  [ex[\"ta\"] for ex in example[\"translation\"]]\n  return tokenizer(english_text, text_target=tamil_text, max_length=128, truncation=True)\ntokenized_data= split_datasets.map(preprocess, batched=True, remove_columns=split_datasets[\"train\"].column_names)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bc975eea-24c7-4f6f-bdc5-d8801dfe780e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Train the model\nfrom transformers import AutoModelForSeq2SeqLM\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\").to(device)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ee477985-e8ea-4ad4-b262-21e892435e68","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from transformers import DataCollatorForSeq2Seq\ndata_collate = DataCollatorForSeq2Seq(tokenizer, model, label_pad_token_id=tokenizer.pad_token_id, return_tensors=\"pt\")\ndata_collate"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e7eaf902-8041-4cf0-a59f-3512106ec6e5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Data Loaders\nfrom torch.utils.data import DataLoader\ntrain_dataloader = DataLoader(tokenized_data[\"train\"], collate_fn = data_collate, shuffle=True, batch_size = 32)\ntest_dataloader = DataLoader(tokenized_data[\"test\"], collate_fn= data_collate, batch_size = 32)\nlen(train_dataloader)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c49be367-97c3-4fb1-85c0-3d116f7df6f1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# try lower learning rate\n# how does learning rate change\n# log both start learning rate (param) and scheduled learning rate\n# early stopping (metric and threshold) if metric does not improve after 5 batches/epochs, "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"171c10a2-c00d-4e24-9f6e-6a3a6f0ccbef","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from transformers import get_scheduler\nfrom tqdm.auto import tqdm\nfrom torch.optim import AdamW\nfrom torch.nn import CrossEntropyLoss\nfrom transformers import EarlyStoppingCallback\n\nexperiment_name = \"/Users/Ajay.Kamalakannan@edelmandxi.com/PyTorch_Projects/text-translation-experiment\"\nimport mlflow\n#mlflow.create_experiment(experiment_name)\nmlflow.set_experiment(experiment_name)\nloss = CrossEntropyLoss()\noptimizer = AdamW(model.parameters(), lr=1e-3)\nepochs = 1\nlr_scheduler = get_scheduler(name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=epochs*len(train_dataloader))\nprogress_bar = tqdm(range(epochs*len(train_dataloader)))\nmodel.train()\ntotal_loss = 0\nfrom accelerate import Accelerator\naccelerator = Accelerator()\ntrain_dataloader, test_dataloader, model, optimizer = accelerator.prepare(train_dataloader, test_dataloader, model, optimizer)\n\n#dir(lr_scheduler.optimizer)\nfor pg in lr_scheduler.optimizer.param_groups:\n  lr = pg['lr']\n  print(lr)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b6899dbf-d0f3-4710-9ee6-8bc6600762f1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from sacrebleu import raw_corpus_bleu\n\n# 2 experiments (training and testing)\n# without fine tuning only (testing)\n# 1 experiment with 2 separate runs (1 per hypothesis) (1 experiment for each project, when logging, training runs, \n#(load base model, calculate performance metrics as single run)\n# (train loop with fine-tuning and show)\n# couple of epochs for specific task"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2589cbca-8c76-40d7-a305-3c6736e8f267","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Tracking part of MLFlow\n# Artifact or Model that is Deployment- Ready (pyfunc)\n# attach and detach cluster\nimport evaluate\nmetric = evaluate.load(\"sacrebleu\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a5b3a212-1c69-4f3c-923f-4e2497590293","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["accumulation_steps = 10\nwith mlflow.start_run():\n  for epoch in range(epochs):\n    for i, batch in enumerate(train_dataloader):\n      outputs = model(input_ids=batch[\"input_ids\"], labels=batch[\"labels\"])\n      loss_values = outputs.loss\n      print(f\"Loss: {loss_values.item()}\")\n      mlflow.log_metric(\"Learning Rate\", lr)\n      mlflow.log_metric(\"Training Loss\", loss_values.item())\n      total_loss += loss_values.item()\n      optimizer.zero_grad()\n      accelerator.backward(loss_values)\n      optimizer.step()\n      lr_scheduler.step()\n      progress_bar.update(1)\n      \n    for batch in tqdm(test_dataloader):\n      with torch.inference_mode():\n\n        gen_tokens = accelerator.unwrap_model(model).generate(\n            batch[\"input_ids\"],\n            attention_mask = batch[\"attention_mask\"],\n            max_length=128,\n        )\n        #print(batch)\n        labels = batch[\"labels\"]\n\n        gen_tokens = accelerator.pad_across_processes(\n            gen_tokens, dim=1, pad_index = tokenizer.pad_token_id\n        )\n        labels = accelerator.pad_across_processes(\n            labels, dim = 1, pad_index = tokenizer.pad_token_id\n        )\n        #print(gen_tokens, labels)\n        #print(\"Before Gathering\")\n        pred_gathered =accelerator.gather(gen_tokens)\n        labels_gathered =accelerator.gather(labels)\n        #print(\"After Gathering\")\n        #print(pred_gathered, labels_gathered)\n        #test_outputs = model(input_ids=batch[\"input_ids\"], labels=batch[\"labels\"])\n\n        #print(test_outputs[0], test_outputs[1])\n        #preds = test_outputs[\"input_ids\"]\n        #labels = test_outputs[\"labels\"]\n\n        pred_gathered = pred_gathered.cpu().numpy()\n        labels_gathered = labels.cpu().numpy()\n        decoded_preds = tokenizer.batch_decode(pred_gathered, skip_special_tokens=True)\n        decoded_labels = tokenizer.batch_decode(labels_gathered, skip_special_tokens=True)\n        #print(decoded_preds, decoded_labels)\n\n        decoded_preds = [pred.strip() for pred in decoded_preds]\n        decoded_labels = [[labels.strip()] for labels in decoded_labels]\n\n        metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n  results = metric.compute()\n  print(f\"The average bleu score: {results['score']}\")\n  mlflow.log_metric(\"Bleu Metric\", results['score'])\n  avg_loss = total_loss/len(train_dataloader)\n  print(f\"Average Training Loss: {total_loss/len(train_dataloader)}\")\n  mlflow.log_metric(\"Average Training Loss\", avg_loss)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c53bf264-fe1a-4660-8751-a8745ce2756b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"51321f1f-8ece-4fde-a47d-3ce4083923fe","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Text_Translation_HF","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4054187928511780}},"nbformat":4,"nbformat_minor":0}
